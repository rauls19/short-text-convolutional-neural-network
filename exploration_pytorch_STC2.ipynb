{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from torchinfo import summary\n",
    "import gensim\n",
    "import numpy as np\n",
    "import torchtext\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_abstracts = pd.read_csv('data/arxiv_data_210930-054931.csv', sep=',')\n",
    "print(data_abstracts.shape)\n",
    "print(data_abstracts.columns)\n",
    "data_abstracts.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summaries = pd.read_csv('data/arxiv_data.csv', sep=',')\n",
    "print(data_summaries.shape)\n",
    "print(data_summaries.columns)\n",
    "data_summaries.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example patterns\n",
    "# patterns = re.compile(r'([^\\w\\s]\\s+)|(this badge holder)|(this badge earner)|(this earner)|(the badge holder)|(the badge earner)', flags=re.IGNORECASE)\n",
    "patterns = re.compile(r'([^\\w\\s]\\s+)', flags=re.IGNORECASE)\n",
    "def clean_abstract(text):\n",
    "    text = patterns.sub(r' ', text)\n",
    "    text = re.sub(r' +', ' ', text).strip()\n",
    "    tk_text = nltk.word_tokenize(text)\n",
    "    new_text = ['' if len(x) < 2 or (x.isnumeric() and len(x) > 4) else x.lower() for x in tk_text]\n",
    "    return ' '.join(new_text)\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield nltk.word_tokenize(text)\n",
    "\n",
    "def generate_vocabulary(data):\n",
    "    vocab = torchtext.vocab.build_vocab_from_iterator(yield_tokens(data), specials=[\"<unk>\"])\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def generate_embedding_matrix(vocabulary: torchtext.vocab.Vocab, embed_model):\n",
    "    embedding_matrix = np.zeros((vocabulary.__len__(), embed_model.vector_size))\n",
    "    for word, index in vocabulary.get_stoi().items():\n",
    "        try:\n",
    "            embedding_matrix[index] = embed_model[word]\n",
    "        except:\n",
    "            continue\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_abstracts['abs_cleaned'] = data_abstracts['abstracts'].apply(clean_abstract)\n",
    "display(data_abstracts.head(3))\n",
    "glove_model = gensim.models.KeyedVectors.load_word2vec_format('data/glove.6B.50d.txt', binary=False, no_header=True)\n",
    "weights = torch.FloatTensor(glove_model.vectors)\n",
    "vocab = generate_vocabulary(data_abstracts['abs_cleaned'].values)\n",
    "embed_matrix = generate_embedding_matrix(vocab, glove_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_text = data_abstracts['abs_cleaned'].apply(lambda x: len(x.split())).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_codes(target):\n",
    "    \"\"\"\n",
    "    Convert source to binary code\n",
    "    :param target: array of values to convert to binary codes\n",
    "    :return: binary codes\n",
    "    \"\"\"\n",
    "    median = np.median(target, axis=1).reshape((target.shape[0], 1))\n",
    "    binary = np.zeros(shape=np.shape(target))\n",
    "    binary[target > median] = 1\n",
    "    return binary\n",
    "\n",
    "def low_dimensional_vector(term_freq_mat, embedding_matrix=None, lwdv='LSA'):\n",
    "    \"\"\"\n",
    "    Generate Low dimensional vector\n",
    "    :param tokenizer: tokenizer object\n",
    "    :param sequences_full: sequences of text\n",
    "    :param embedding_matrix: embedded matrix\n",
    "    :param lwdv: Type of low dimensional vector\n",
    "    :return: low dimensional vector\n",
    "    \"\"\"\n",
    "    if lwdv == 'LE':  # Laplacian Eigenmaps (LE) # MEMORY EXPENSIVE\n",
    "        le = SpectralEmbedding(n_components=300)  # explore parameters\n",
    "        dim_reduction_le = le.fit_transform(term_freq_mat)\n",
    "        return dim_reduction_le\n",
    "    elif lwdv == 'AE':  # Average embedding (AE) # MEMORY EXPENSIVE\n",
    "        denom = 1 + np.sum(term_freq_mat, axis=1)[:, None]\n",
    "        normed_tfidf = term_freq_mat / denom\n",
    "        average_embeddings = np.dot(normed_tfidf, embedding_matrix)\n",
    "        return average_embeddings\n",
    "    elif lwdv == 'LSA':  # LSA\n",
    "        lsa = TruncatedSVD(n_components=300, algorithm=\"arpack\")\n",
    "        dim_reduction_lsa = lsa.fit_transform(term_freq_mat)\n",
    "        return dim_reduction_lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(nltk.tokenize.word_tokenize(x))\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(data_abstracts[\"abs_cleaned\"].values)\n",
    "lsa = TruncatedSVD(n_components=300, algorithm=\"arpack\")\n",
    "lsa.fit(tfidf.transform(data_abstracts[\"abs_cleaned\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If CUDA available, we can send to the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch_stc(batch: list):\n",
    "    text_list = []\n",
    "    B = []\n",
    "    for _text in batch:\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        processed_text = nn.functional.pad(processed_text, pad=(0, max_words_text-processed_text.shape[0])) #(padding_left,padding_right)\n",
    "        text_list.append(processed_text)\n",
    "    Y = lsa.transform(tfidf.transform(batch)) # Low dimenstionality reduction LSA\n",
    "    B = torch.tensor(binary_codes(Y), dtype=torch.float32)\n",
    "    text_list = torch.nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
    "    return  B.to(device), text_list.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_idx, x_test_idx = train_test_split(data_abstracts['abs_cleaned'].index,test_size=0.3, shuffle=True, random_state=42)\n",
    "x_val_idx, x_test_idx = train_test_split(x_test_idx, test_size=0.66, shuffle=True, random_state=42)\n",
    "BATCH_SIZE = 25\n",
    "train_dataloader_stc = DataLoader(data_abstracts.loc[x_train_idx][\"abs_cleaned\"].values, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_stc, num_workers=0)\n",
    "test_dataloader_stc = DataLoader(data_abstracts.loc[x_test_idx][\"abs_cleaned\"].values, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_stc, num_workers=0)\n",
    "validate_dataloader_stc = DataLoader(data_abstracts.loc[x_val_idx][\"abs_cleaned\"].values, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch_stc, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataloader_stc, \"train_dataloader_stc.bin\")\n",
    "torch.save(test_dataloader_stc, \"test_dataloader_stc.bin\")\n",
    "torch.save(validate_dataloader_stc, \"validate_dataloader_stc.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.dropout = torch.nn.Dropout(p= 0.05) # Dropout\n",
    "        self.lstm = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True) #lstm\n",
    "\n",
    "        self.fc_1 =  torch.nn.Linear(hidden_size, hidden_size//2) #fully connected 1\n",
    "        self.fc = torch.nn.Linear(hidden_size//2, 1) #fully connected last layer\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h_0 = torch.autograd.Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = torch.autograd.Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        _, (hn, cn) = self.lstm(x, (h_0.detach(), c_0.detach())) #lstm with input, hidden, and internal state\n",
    "        hn_fs = hn.view(self.num_layers, x.size(0), self.hidden_size)[-1] #reshaping the data for Dense layer next\n",
    "        out = self.dropout(hn_fs) # Dropout\n",
    "        out = self.fc_1(out) # Dense\n",
    "        out = self.relu(out) # Activation\n",
    "        out = self.fc(out) # Dense\n",
    "        out = self.relu(out) # Activation\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModelCNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size) -> None:\n",
    "        super().__init__()\n",
    "        # Define options for every element\n",
    "        self.conv1d = torch.nn.Conv1d(in_channels, out_channels, kernel_size) # here\n",
    "        self.maxpool = torch.nn.MaxPool1d(kernel_size=kernel_size) # here\n",
    "        self.fc = torch.nn.Linear(in_features=20, out_features=20)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Review\n",
    "        x = self.conv1d(x)\n",
    "        # x = self.tanh(x)\n",
    "        x = torch.nn.functional.tanh(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        # x = self.sigmoid(x)\n",
    "        x = torch.nn.functional.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(SimpleModelCNN(100, 100, 5)) # model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_STC(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, filter_size, num_filter, num_classes, dropout) -> None:\n",
    "        super(CNN_STC, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) # check padding\n",
    "        self.conv_1d = nn.Conv1d(in_channels=embed_dim, out_channels=num_filter, kernel_size=filter_size)\n",
    "        self.pool_1d = nn.MaxPool1d(kernel_size=filter_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        in_lin = int(num_filter * ((embed_dim / filter_size) - 1))\n",
    "        self.fc = nn.Linear(3000, num_classes) # based on the .view(size(0), -1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv_1d(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.pool_1d(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_stc = CNN_STC(len(vocab), 284, 4, 25, 300, 0.5)\n",
    "summary(model_cnn_stc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01  # learning rate\n",
    "\n",
    "optimizer = torch.optim.Adam(model_cnn_stc.parameters(), lr=LR)\n",
    "loss_function = torch.nn.BCELoss() # Input: (N, C), N=data, C=Total classes, Target:(N)\n",
    "\n",
    "def train(epoch, data_loader, model):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 50\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text) in enumerate(data_loader):\n",
    "        optimizer.zero_grad(True)\n",
    "        predicted_label = model(text)\n",
    "        loss = loss_function(predicted_label, label.squeeze_())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        # total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_acc += (predicted_label == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\"| epoch {:3d} | {:5d}/{:5d} batches | accuracy {:8.3f} | elapsed time {:2.2f}s\".format(epoch, idx, len(data_loader), \n",
    "                                                                                                        total_acc / total_count, \n",
    "                                                                                                        elapsed))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(data_loader, model):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad(): # very important, if not we update the gradient\n",
    "        for idx, (label, text) in enumerate(data_loader):\n",
    "            predicted_label = model(text)\n",
    "            loss = loss_function(predicted_label, label.squeeze_())\n",
    "            # total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_acc += (predicted_label == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count\n",
    "\n",
    "def train_model(epochs, train_dataloader, validation_data_loader, model, total_accu=None):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(epoch, train_dataloader, model)\n",
    "        accu_val = evaluate(validation_data_loader, model)\n",
    "        print(\"-\" * 59)\n",
    "        print(\"| end of epoch {:3d} | time: {:5.2f}s | valid accuracy {:8.3f} \".format(epoch, time.time() - epoch_start_time, accu_val))\n",
    "        print(\"-\" * 59)\n",
    "\n",
    "def predict(model, data_loader):\n",
    "    predicted_label = []\n",
    "    with torch.no_grad(): # very important, if not we update the gradient\n",
    "        for idx, (label, text) in enumerate(data_loader):\n",
    "            predicted_label.append(model(text).argmax(1))\n",
    "    return torch.cat(predicted_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(1, train_dataloader_stc, validate_dataloader_stc, model_cnn_stc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu_test = evaluate(test_dataloader_stc, model_cnn_stc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_STC_V2(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, filter_size, num_filter, num_classes, dropout) -> None:\n",
    "        super(CNN_STC_V2, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) # check padding\n",
    "        self.conv_1d_1 = nn.Conv1d(in_channels=embed_dim, out_channels=num_filter, kernel_size=filter_size)\n",
    "        self.conv_1d_2 = nn.Conv1d(in_channels=num_filter, out_channels=num_filter//2, kernel_size=filter_size) # (in_channels=embed_dim, out_channels=num_filter, kernel_size=filter_size)\n",
    "        self.max_pool_1d = nn.MaxPool1d(kernel_size=filter_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(3000, num_classes) # based on the .view(size(0), -1)\n",
    "        self.filter_size = filter_size\n",
    "        self.L = 2 # Number of Conv defined\n",
    "    \n",
    "    def folding(self, x):\n",
    "        if x.size()[0] % 2 != 0:\n",
    "            x = torch.stack([x[i] + x[i+1] for i in range(0, x.size()[0]-1, 2)]) # Every two rows\n",
    "            x_last = x[x.size()[0]-1]\n",
    "            x_last = x_last.view(1, x_last.size()[0], x_last.size()[1])\n",
    "            x = torch.cat((x, x_last), 0)\n",
    "        else:\n",
    "            x = torch.stack([x[i] + x[i+1] for i in range(0, x.size()[0], 2)]) # Every two rows\n",
    "        return x\n",
    "\n",
    "    def dynamic_k_maxpooling(self, x, l, dim):\n",
    "        kl = int(max(self.filter_size, (self.L-l)/self.L*x.size()[2]))\n",
    "        index = x.topk(kl, dim = dim)[1].sort(dim = dim)[0]\n",
    "        return x.gather(dim, index)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv_1d_1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dynamic_k_maxpooling(x, 1, 2)\n",
    "        x = self.conv_1d_2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.folding(x)\n",
    "        x = self.max_pool_1d(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "# Cleaning patterns\n",
    "patterns = re.compile(r'([^\\w\\s]\\s+)', flags=re.IGNORECASE)\n",
    "\n",
    "def clean_abstract(text):\n",
    "        text = patterns.sub(r' ', text)\n",
    "        text = re.sub(r' +', ' ', text).strip()\n",
    "        tk_text = nltk.word_tokenize(text)\n",
    "        new_text = ['' if len(x) < 2 or (x.isnumeric() and len(x) > 4) else x.lower() for x in tk_text]\n",
    "        return ' '.join(new_text)\n",
    "\n",
    "# Read data\n",
    "data_abstracts = pd.read_csv('data/arxiv_data_210930-054931.csv', sep=',')\n",
    "# Clean data\n",
    "data_abstracts['abs_cleaned'] = data_abstracts['abstracts'].apply(clean_abstract)\n",
    "# Split data into training, validating, testing\n",
    "x_train_idx, x_test_idx = train_test_split(data_abstracts['abs_cleaned'].index,test_size=0.3, shuffle=True, random_state=42)\n",
    "x_val_idx, x_test_idx = train_test_split(x_test_idx, test_size=0.66, shuffle=True, random_state=42)\n",
    "\n",
    "# Load Glove Model if needed\n",
    "# glove_model = gensim.models.KeyedVectors.load_word2vec_format('data/glove.6B.50d.txt', binary=False, no_header=True)\n",
    "# weights = torch.FloatTensor(glove_model.vectors)\n",
    "# embed_matrix = generate_embedding_matrix(vocab, glove_model)\n",
    "\n",
    "# Generate vocabulary\n",
    "utils.vocab = utils.generate_vocabulary(data_abstracts['abs_cleaned'].values)\n",
    "# Vocab + tokenization function\n",
    "utils.text_pipeline = lambda x: utils.vocab(nltk.tokenize.word_tokenize(x))\n",
    "# Get maximum words in a sentence, used for padding\n",
    "utils.max_words_text = data_abstracts['abs_cleaned'].apply(lambda x: len(x.split())).max()\n",
    "# Fit TF-IDF and LSA\n",
    "utils.tfidf.fit(data_abstracts[\"abs_cleaned\"].values)\n",
    "utils.lsa.fit(utils.tfidf.transform(data_abstracts[\"abs_cleaned\"].values))\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 25\n",
    "train_dataloader_stc = DataLoader(data_abstracts.loc[x_train_idx][\"abs_cleaned\"].values, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                                collate_fn=utils.collate_batch_stc)\n",
    "test_dataloader_stc = DataLoader(data_abstracts.loc[x_test_idx][\"abs_cleaned\"].values, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                                collate_fn=utils.collate_batch_stc)\n",
    "validate_dataloader_stc = DataLoader(data_abstracts.loc[x_val_idx][\"abs_cleaned\"].values, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                                        collate_fn=utils.collate_batch_stc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from models import CNN_STC\n",
    "import train as tr\n",
    "model_cnn_stc = CNN_STC(len(utils.vocab), utils.max_words_text, 4, 25, 300, 0.5)\n",
    "print(summary(model_cnn_stc))\n",
    "# Define Variables\n",
    "tr.LR = 0.01 # By defult, customisable\n",
    "tr.optimizer = torch.optim.Adam(model_cnn_stc.parameters(), lr=tr.LR)\n",
    "tr.loss_function = torch.nn.BCELoss() # Input: (N, C), N=data, C=Total classes, Target:(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.train_model(epochs=1, train_dataloader=train_dataloader_stc, validation_data_loader=validate_dataloader_stc, model=model_cnn_stc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_STC_V2(len(utils.vocab), utils.max_words_text, 4, 25, 300, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# ------------------------------------- #\n",
    "# Define training parameters\n",
    "# ------------------------------------- #\n",
    "LR = 0.01  # learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_function = torch.nn.BCELoss()\n",
    "\n",
    "def train(epoch, data_loader, model):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 50\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text) in enumerate(data_loader):\n",
    "        optimizer.zero_grad(True)\n",
    "        predicted_label = model(text)\n",
    "        loss = loss_function(predicted_label, label.squeeze_())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        # total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_acc += (predicted_label == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\"| epoch {:3d} | {:5d}/{:5d} batches | accuracy {:8.3f} | elapsed time {:2.2f}s\".format(epoch, idx, len(data_loader), \n",
    "                                                                                                        total_acc / total_count, \n",
    "                                                                                                        elapsed))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(data_loader, model):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad(): # very important, if not we update the gradient\n",
    "        for idx, (label, text) in enumerate(data_loader):\n",
    "            predicted_label = model(text)\n",
    "            loss = loss_function(predicted_label, label.squeeze_())\n",
    "            # total_acc += (predicted_label.argmax(1) == label).sum().item() # If we have one output possible class\n",
    "            total_acc += (predicted_label == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count\n",
    "\n",
    "def train_model(epochs, train_dataloader, validation_data_loader, model, total_accu=None):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(epoch, train_dataloader, model)\n",
    "        accu_val = evaluate(validation_data_loader, model)\n",
    "        print(\"-\" * 59)\n",
    "        print(\"| end of epoch {:3d} | time: {:5.2f}s | valid accuracy {:8.3f} \".format(epoch, time.time() - epoch_start_time, accu_val))\n",
    "        print(\"-\" * 59)\n",
    "\n",
    "def predict(model, data_loader):\n",
    "    predicted_label = []\n",
    "    with torch.no_grad(): # very important, if not we update the gradient\n",
    "        for idx, (label, text) in enumerate(data_loader):\n",
    "            predicted_label.append(model(text).argmax(1))\n",
    "    return torch.cat(predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "total_acc, total_count = 0, 0\n",
    "log_interval = 50\n",
    "for epoch in range(1, 1 + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(epoch, train_dataloader_stc, model)\n",
    "        accu_val = evaluate(validate_dataloader_stc, model)\n",
    "        print(\"-\" * 59)\n",
    "        print(\"| end of epoch {:3d} | time: {:5.2f}s | valid accuracy {:8.3f} \".format(epoch, time.time() - epoch_start_time, accu_val))\n",
    "        print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4, 5, 10)\n",
    "s = x.size()[2]\n",
    "k_ll = ((2 - 1) / 2) * s\n",
    "k_l = round(max(4, np.ceil(k_ll)))\n",
    "out = torch.adaptive_max_pool1d(x, k_l)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.rand(4, 5, 10)\n",
    "index = x.topk(4, dim = 2)[1].sort(dim = 2)[0]\n",
    "y = x.gather(2, index)\n",
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f5a157acd88d3c0e1ad279d81cbcba6984ff8b37ec85aaa44158ab13d418bbb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
